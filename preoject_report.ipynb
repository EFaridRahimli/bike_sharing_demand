{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report: Predict Bike Sharing Demand with AutoGluon Solution\n",
    "### Farid Rahimli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Training\n",
    "### What did you realize when you tried to submit your predictions? What changes were needed to the output of the predictor to submit your results?\n",
    "#### *Initially the predictor did not accept the test set due to difference in columns train set had columns for casual and registered users however, test set was lacking these columns. The training would not have worked unless these columns were dropped or synthetically added to the test. Latter would be very inaccurate therefore my decision was to drop the columns from the training set. After conducting data visualizations specifically of the target variable distribution it was evident that the variable was highly skewed to the right. This meant that lower values were prevalent in the training set. To eliminate this data was transformed with log1. Result of this was bell shaped distribution which was much more applicable. During submission to kaggle some rmse values were negative which cause an error. This were eliminated by transforming the output to absolute values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What was the top ranked model that performed?\n",
    "#### *Top ranked model for all three stages of training was WeightedEnsembleL3. The lowest error was on the tertiary training according to AutuGluon evaluation metrics but according to Kaggle the lowest error was on the secondary training*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis and feature creation\n",
    "### What did the exploratory analysis find and how did you add additional features?\n",
    "#### *EDA Findings*\n",
    "#### *First and one of the most important findings during EDA was that the target variable was skewed and needed to be balanced. Secondary important finding was that the additional datetime features that were extracted and added to the training and test data had strong impact on rentals. Moreover, EDA showed that amount of rentals were peaking during the rush hour times. Lastly, temperature and weather had significant impact on rentals at all times.*\n",
    "#### *Features Added*\n",
    "#### *Additional features such as: hour, day, month, weekday, year were extracted. 2 bool type features were added as well such as: weekday_bool and rushhour_bool. Polynomial features that multiples 2 existing features were added: interaction between humidity and temperature at the vector of temp_humidity, interaction between windspeed and temperature and interaction between hour and workday at the vector of workday_hour. Lastly to ease the process of training weather, year, weekday and month features were set as categorical values.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much better did your model preform after adding additional features and why do you think that is?\n",
    "#### *Initial Training score value according to AutoGluon evaluation metrics was ~ -1*\n",
    "#### *Secondary Training score value according to AutoGluon evaluation metric after Feature engineering was ~ -0.5300*\n",
    "#### *Tertiary Training score value according to AutoGluon evaluation metric after Feature engineering and Hyperparameter tuning was ~ -0.26923*\n",
    "\n",
    "### NOTE ALL THESE VALUES ARE AFTER LOG TRANSFORMATION TO BALANCE THE VALUES AND CREATE BETTER VISUALIZATION\n",
    "\n",
    "#### *Initial Training score value according to AutoGluon evaluation metrics was ~ -138.3919*\n",
    "#### *Secondary Training score value according to AutoGluon evaluation metric after Feature engineering was ~ -0.69917*\n",
    "#### *Tertiary Training score value according to AutoGluon evaluation metric after Feature engineering and Hyperparameter tuning was ~ -0.313665*\n",
    "\n",
    "### NOTE ALL THESE VALUES ARE ORIGINAL VALUES BEFORE LOG TRANSFORMATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper parameter tuning\n",
    "### How much better did your model preform after trying different hyper parameters?\n",
    "#### *After tuning the score were improved drastically Kaggle score improved from ~ 1.8 to ~ 0.4 AutoGluon RMSE improved from ~138 (non log) to ~0.3 (non log)*\n",
    "\n",
    "#### *Improvements were possible due to additional hyperparameters for each model type AutoGluon uses (added to GBM, XGBoost, Random Forest and Neural Network)*\n",
    "#### *Added best+prese quality attribute to choose best model*\n",
    "#### *Ensemble models prevailed in terms of accuracy*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you were given more time with this dataset, where do you think you would spend more time?\n",
    "#### *Additional feature engineering could improve the results such as extracting and constructing new feature for holidays as well as lag feature could have been implemented due to the timeseries aspect of the data*\n",
    "#### *More hyperparameter tuning could improve results as well as implementing Bayesian hyperparameter optimization*\n",
    "#### *Using different deep learning models to train*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table with the models you ran, the hyperparameters modified, and the kaggle score.\n",
    "| Model Stage     |   Score (RMSE) |   Score (Kaggle) | Changes                      |\n",
    "|:----------------|---------------:|-----------------:|:-----------------------------|\n",
    "| Initial Model   |      -1        |          1.86413 | Default                      |\n",
    "| Secondary Model |      -0.503007 |          0.42063 | Feature Engineered \\ Cleaned |\n",
    "| Tertiary Model  |      -0.269237 |          0.42091 | Tuned, Additional Features   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line plot showing the top model score for the three (or more) training runs during the project.\n",
    "![scores(RMSE)](graphs/scoresrmse.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a line plot showing the top kaggle score for the three (or more) prediction submissions during the project.\n",
    "![Kagglescores](graphs/kaggle%20scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The best model was Weighted EnsembleL2 after hyperparameter tuning and Feature Engineering*\n",
    "#### *RMSE scores improved substantially after implementing feature extractions and adding new features*\n",
    "#### *Specifically Feature Extraction decreased the score from -189 to 0.5 (approximately)*\n",
    "#### *Hyperparameter tuning and additional extraction / creation of polynomial features improved the score further from -0.5 to -0.2 (approximately)*\n",
    "\n",
    "#### *Implementation of additional features can improve scores extracting holidays, additional weather information and lag*\n",
    "#### *Moreover, bayesian hyperparameter tuning can show improvements in accuracy to some degree*\n",
    "#### *More complex deep learning models can prove to be more accurate*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
